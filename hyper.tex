In this chapter I briefly discuss the concept of global optimization also known as hyper-parameters optimization. Hyper-parameters optimization is one of the few concepts in machine learning literature lacking intuition, theoretical understanding and practical guidelines. This is due to the intrinsic nature of the problem since hyper-parameters optimization is built on top of parameters optimization i.e learning and as such demands higher computational resources. 

In this chapter, I discuss the hyper-parameters optimization using Gaussian processes. I apply the concept to practical test using python module GPyOpt [21]. More specifically, I use GPyOpt which is a python module that uses Gaussian processes to find out the best hyper-parameters for two training algorithms Adam and Successive convex approximation within fixed number of iterations. I then use these hyper-parameters in Adam and Successive convex approximation and compare their performance results which are discussed in detail in the next chapter. 

\section {Hyper-parameters Optimization}

The hyper-parameters optimization also referred to as global optimization points to finding the best set of parameters that any arbitrary function takes to process some input. Hyper-parameters optimization is one of the most challenging problems in machine learning. This is due to the fact that the under lying machine learning model e.g neural network takes a lot of time for it's own learning. Thus, it's difficult to find the best hyper-parameters within very few iterations and almost always, the default hyper-parameters are used.

Hyper-parameters Optimization in machine learning is a subject lacking theoretical insight and empirical evidence. Untill recently, grid search was the most obvious choice regarding hyper parameter optimization.  It simply meant that the designer of the machine learning algorithm presented some choices of the hyper-parameters based on his/her previous knowledge. Randomly changing the hyper-parameters was another choice however both were unfeasible. This was due to the fact that the former required too much technical expertise on the subject while the latter lacked meaningful intuition. Recently, some work on hyper-parameters optimization using non parametric models such as Gaussian processes has gained considerable attention.

\section {Hyper-parameters Optimization via GPyOpt}

In this section, I discuss the hyper-parameters optimization for two algorithms namely the state of the art Adam and the newly proposed Successive convex approximation. More specifically, I use Gaussian processes (GP) [19] which are implemented in Python module called GPyOpt [20].

Gaussian processes (GP) [19] are defined as a finite collection of jointly Gaussian distributed random variables. For regression problems these random variables represent the values of a function $f(\textbf{x})$ at input points $\textbf{x}$. Prior beliefs about the properties of the latent function are encoded by the mean function $m(\textbf{x})$ and co variance function $k(\textbf{x}, \textbf{x}_0)$. Thereby, all function classes that share the same prior assumptions are covered and inferences can be made directly in function space. 

The Python module GPyOPt implements the Gaussian processes for hyper-parameters optimization [20]. It requires the definition of a function, the hyper-parameters to be optimized, and some prior belief on the optimal value of the parameters. Using this information, it tries to optimize the hyper-parameters of the function. 

In the case of Adam, there are four hyper-parameters as presented in (2.17), (2.18) and (2.19). For Successive convex approximation, I try to optimize the initial step size $\alpha_0$, the scalar to update the gradients $\rho$ and the term to make the optimization problem strongly convex $\tau$ as presented in (3.3) and (3.4). The function to be optimized is the mean classification score on test data set. The initial priors are set randomly. The entire process is described below.

I run the GPyOpt for 50 iterations for both training algorithms Adam and Successive convex approximation. The initial beliefs are fed randomly. At every iteration, GPyOpt uses the prior to guess the best possible hyper-parameters. It uses those parameters in the training algorithm and runs the training process. Once the learning process ends, it calculates the mean classification score on test data set and saves the hyper-parameters and score in the memory. In the next iteration, it uses the initially provided and the previously used hyper-parameters as priors and repeats the same process. After 50 iterations, it returns the best hyper-parameters based on the best mean classification score. This is done for all 8 experiments that are mentioned above.

More specifically, the process is done for two data sets i.e. MNIST and Fashion MNIST, for two training algorithms i.e. Adam and Successive convex approximation and for two input representations e.g. standard and column wise. Once the best hyper-parameters are learned within 50 iterations for each of the 8 choices, I use those hyper-parameters to finally compare the performance of both training algorithms. The experimental results on the performance comparison of Adam and Successive convex approximation with two settings i.e. with default hyper-parameters and with hyper-parameters received from Gaussian Processes (GP) as implemented in GPyOpt are discussed.

